{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 60.0,
  "eval_steps": 500,
  "global_step": 8280,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.7246376811594203,
      "grad_norm": 1.4624035358428955,
      "learning_rate": 0.00019758454106280193,
      "loss": 2.3418,
      "step": 100
    },
    {
      "epoch": 1.4492753623188406,
      "grad_norm": 1.471898078918457,
      "learning_rate": 0.00019516908212560387,
      "loss": 1.982,
      "step": 200
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 1.5682518482208252,
      "learning_rate": 0.0001927536231884058,
      "loss": 1.8379,
      "step": 300
    },
    {
      "epoch": 2.898550724637681,
      "grad_norm": 1.4712086915969849,
      "learning_rate": 0.00019033816425120773,
      "loss": 1.703,
      "step": 400
    },
    {
      "epoch": 3.6231884057971016,
      "grad_norm": 1.5160478353500366,
      "learning_rate": 0.00018792270531400965,
      "loss": 1.58,
      "step": 500
    },
    {
      "epoch": 4.3478260869565215,
      "grad_norm": 1.7900810241699219,
      "learning_rate": 0.0001855072463768116,
      "loss": 1.4923,
      "step": 600
    },
    {
      "epoch": 5.072463768115942,
      "grad_norm": 2.2790117263793945,
      "learning_rate": 0.0001830917874396135,
      "loss": 1.3839,
      "step": 700
    },
    {
      "epoch": 5.797101449275362,
      "grad_norm": 2.124589443206787,
      "learning_rate": 0.00018067632850241546,
      "loss": 1.2775,
      "step": 800
    },
    {
      "epoch": 6.521739130434782,
      "grad_norm": 2.3744165897369385,
      "learning_rate": 0.0001782608695652174,
      "loss": 1.1658,
      "step": 900
    },
    {
      "epoch": 7.246376811594203,
      "grad_norm": 2.33927321434021,
      "learning_rate": 0.00017584541062801935,
      "loss": 1.1338,
      "step": 1000
    },
    {
      "epoch": 7.971014492753623,
      "grad_norm": 2.8315541744232178,
      "learning_rate": 0.00017342995169082126,
      "loss": 1.0954,
      "step": 1100
    },
    {
      "epoch": 8.695652173913043,
      "grad_norm": 2.2422878742218018,
      "learning_rate": 0.0001710144927536232,
      "loss": 0.9584,
      "step": 1200
    },
    {
      "epoch": 9.420289855072463,
      "grad_norm": 2.0060250759124756,
      "learning_rate": 0.00016859903381642513,
      "loss": 0.92,
      "step": 1300
    },
    {
      "epoch": 10.144927536231885,
      "grad_norm": 2.3614230155944824,
      "learning_rate": 0.00016618357487922707,
      "loss": 0.9,
      "step": 1400
    },
    {
      "epoch": 10.869565217391305,
      "grad_norm": 3.043025255203247,
      "learning_rate": 0.000163768115942029,
      "loss": 0.8095,
      "step": 1500
    },
    {
      "epoch": 11.594202898550725,
      "grad_norm": 3.064929962158203,
      "learning_rate": 0.00016135265700483093,
      "loss": 0.7428,
      "step": 1600
    },
    {
      "epoch": 12.318840579710145,
      "grad_norm": 3.4873600006103516,
      "learning_rate": 0.00015893719806763285,
      "loss": 0.7072,
      "step": 1700
    },
    {
      "epoch": 13.043478260869565,
      "grad_norm": 2.678950786590576,
      "learning_rate": 0.0001565217391304348,
      "loss": 0.683,
      "step": 1800
    },
    {
      "epoch": 13.768115942028986,
      "grad_norm": 2.424603223800659,
      "learning_rate": 0.0001541062801932367,
      "loss": 0.6134,
      "step": 1900
    },
    {
      "epoch": 14.492753623188406,
      "grad_norm": 2.707082748413086,
      "learning_rate": 0.00015169082125603866,
      "loss": 0.5918,
      "step": 2000
    },
    {
      "epoch": 15.217391304347826,
      "grad_norm": 2.6926233768463135,
      "learning_rate": 0.00014927536231884058,
      "loss": 0.5746,
      "step": 2100
    },
    {
      "epoch": 15.942028985507246,
      "grad_norm": 2.477466106414795,
      "learning_rate": 0.00014685990338164252,
      "loss": 0.5417,
      "step": 2200
    },
    {
      "epoch": 16.666666666666668,
      "grad_norm": 2.1711044311523438,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.4929,
      "step": 2300
    },
    {
      "epoch": 17.391304347826086,
      "grad_norm": 1.958187460899353,
      "learning_rate": 0.00014202898550724638,
      "loss": 0.4911,
      "step": 2400
    },
    {
      "epoch": 18.115942028985508,
      "grad_norm": 2.4400272369384766,
      "learning_rate": 0.00013961352657004833,
      "loss": 0.4669,
      "step": 2500
    },
    {
      "epoch": 18.840579710144926,
      "grad_norm": 2.822251796722412,
      "learning_rate": 0.00013719806763285024,
      "loss": 0.4341,
      "step": 2600
    },
    {
      "epoch": 19.565217391304348,
      "grad_norm": 3.2906322479248047,
      "learning_rate": 0.0001347826086956522,
      "loss": 0.4183,
      "step": 2700
    },
    {
      "epoch": 20.28985507246377,
      "grad_norm": 2.082019090652466,
      "learning_rate": 0.00013236714975845413,
      "loss": 0.3941,
      "step": 2800
    },
    {
      "epoch": 21.014492753623188,
      "grad_norm": 3.3285117149353027,
      "learning_rate": 0.00012995169082125605,
      "loss": 0.3966,
      "step": 2900
    },
    {
      "epoch": 21.73913043478261,
      "grad_norm": 1.988533616065979,
      "learning_rate": 0.00012753623188405797,
      "loss": 0.3555,
      "step": 3000
    },
    {
      "epoch": 22.463768115942027,
      "grad_norm": 2.1212821006774902,
      "learning_rate": 0.0001251207729468599,
      "loss": 0.3619,
      "step": 3100
    },
    {
      "epoch": 23.18840579710145,
      "grad_norm": 1.25868558883667,
      "learning_rate": 0.00012270531400966183,
      "loss": 0.3475,
      "step": 3200
    },
    {
      "epoch": 23.91304347826087,
      "grad_norm": 1.3826615810394287,
      "learning_rate": 0.00012028985507246378,
      "loss": 0.3357,
      "step": 3300
    },
    {
      "epoch": 24.63768115942029,
      "grad_norm": 1.9330142736434937,
      "learning_rate": 0.00011787439613526569,
      "loss": 0.3208,
      "step": 3400
    },
    {
      "epoch": 25.36231884057971,
      "grad_norm": 2.015700578689575,
      "learning_rate": 0.00011545893719806764,
      "loss": 0.321,
      "step": 3500
    },
    {
      "epoch": 26.08695652173913,
      "grad_norm": 1.9216642379760742,
      "learning_rate": 0.00011304347826086956,
      "loss": 0.3032,
      "step": 3600
    },
    {
      "epoch": 26.81159420289855,
      "grad_norm": 2.869722843170166,
      "learning_rate": 0.0001106280193236715,
      "loss": 0.2942,
      "step": 3700
    },
    {
      "epoch": 27.536231884057973,
      "grad_norm": 1.8038835525512695,
      "learning_rate": 0.00010821256038647343,
      "loss": 0.2967,
      "step": 3800
    },
    {
      "epoch": 28.26086956521739,
      "grad_norm": 2.5513265132904053,
      "learning_rate": 0.00010579710144927538,
      "loss": 0.2732,
      "step": 3900
    },
    {
      "epoch": 28.985507246376812,
      "grad_norm": 1.0463318824768066,
      "learning_rate": 0.00010338164251207729,
      "loss": 0.2901,
      "step": 4000
    },
    {
      "epoch": 29.71014492753623,
      "grad_norm": 2.017810106277466,
      "learning_rate": 0.00010096618357487924,
      "loss": 0.2692,
      "step": 4100
    },
    {
      "epoch": 30.434782608695652,
      "grad_norm": 0.9848504066467285,
      "learning_rate": 9.855072463768117e-05,
      "loss": 0.282,
      "step": 4200
    },
    {
      "epoch": 31.159420289855074,
      "grad_norm": 1.5669499635696411,
      "learning_rate": 9.61352657004831e-05,
      "loss": 0.2577,
      "step": 4300
    },
    {
      "epoch": 31.884057971014492,
      "grad_norm": 1.1565535068511963,
      "learning_rate": 9.371980676328503e-05,
      "loss": 0.2645,
      "step": 4400
    },
    {
      "epoch": 32.608695652173914,
      "grad_norm": 1.724678874015808,
      "learning_rate": 9.130434782608696e-05,
      "loss": 0.2513,
      "step": 4500
    },
    {
      "epoch": 33.333333333333336,
      "grad_norm": 2.180964469909668,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.2719,
      "step": 4600
    },
    {
      "epoch": 34.05797101449275,
      "grad_norm": 2.2007858753204346,
      "learning_rate": 8.647342995169082e-05,
      "loss": 0.2652,
      "step": 4700
    },
    {
      "epoch": 34.78260869565217,
      "grad_norm": 20.27253532409668,
      "learning_rate": 8.405797101449276e-05,
      "loss": 0.2618,
      "step": 4800
    },
    {
      "epoch": 35.507246376811594,
      "grad_norm": 11.538875579833984,
      "learning_rate": 8.164251207729469e-05,
      "loss": 0.2744,
      "step": 4900
    },
    {
      "epoch": 36.231884057971016,
      "grad_norm": 1.0325188636779785,
      "learning_rate": 7.922705314009662e-05,
      "loss": 0.2668,
      "step": 5000
    },
    {
      "epoch": 36.95652173913044,
      "grad_norm": 1.523673415184021,
      "learning_rate": 7.681159420289855e-05,
      "loss": 0.2907,
      "step": 5100
    },
    {
      "epoch": 37.68115942028985,
      "grad_norm": 5.088445663452148,
      "learning_rate": 7.439613526570048e-05,
      "loss": 0.2652,
      "step": 5200
    },
    {
      "epoch": 38.405797101449274,
      "grad_norm": 1.8627580404281616,
      "learning_rate": 7.198067632850241e-05,
      "loss": 0.2882,
      "step": 5300
    },
    {
      "epoch": 39.130434782608695,
      "grad_norm": 1.6076563596725464,
      "learning_rate": 6.956521739130436e-05,
      "loss": 0.2658,
      "step": 5400
    },
    {
      "epoch": 39.85507246376812,
      "grad_norm": 2.6680924892425537,
      "learning_rate": 6.714975845410629e-05,
      "loss": 0.266,
      "step": 5500
    },
    {
      "epoch": 40.57971014492754,
      "grad_norm": 2.6102535724639893,
      "learning_rate": 6.473429951690822e-05,
      "loss": 0.2653,
      "step": 5600
    },
    {
      "epoch": 41.30434782608695,
      "grad_norm": 13.21487045288086,
      "learning_rate": 6.231884057971015e-05,
      "loss": 0.2546,
      "step": 5700
    },
    {
      "epoch": 42.028985507246375,
      "grad_norm": 4.399127006530762,
      "learning_rate": 5.990338164251208e-05,
      "loss": 0.2425,
      "step": 5800
    },
    {
      "epoch": 42.7536231884058,
      "grad_norm": 2.739531993865967,
      "learning_rate": 5.748792270531401e-05,
      "loss": 0.2571,
      "step": 5900
    },
    {
      "epoch": 43.47826086956522,
      "grad_norm": 37.048213958740234,
      "learning_rate": 5.507246376811594e-05,
      "loss": 0.2539,
      "step": 6000
    },
    {
      "epoch": 44.20289855072464,
      "grad_norm": 15.074117660522461,
      "learning_rate": 5.265700483091788e-05,
      "loss": 0.2491,
      "step": 6100
    },
    {
      "epoch": 44.927536231884055,
      "grad_norm": 3.706400156021118,
      "learning_rate": 5.024154589371981e-05,
      "loss": 0.252,
      "step": 6200
    },
    {
      "epoch": 45.65217391304348,
      "grad_norm": 1.186621069908142,
      "learning_rate": 4.782608695652174e-05,
      "loss": 0.2523,
      "step": 6300
    },
    {
      "epoch": 46.3768115942029,
      "grad_norm": 2.0265843868255615,
      "learning_rate": 4.541062801932367e-05,
      "loss": 0.2476,
      "step": 6400
    },
    {
      "epoch": 47.10144927536232,
      "grad_norm": 3.436122179031372,
      "learning_rate": 4.299516908212561e-05,
      "loss": 0.2486,
      "step": 6500
    },
    {
      "epoch": 47.82608695652174,
      "grad_norm": 2.3935697078704834,
      "learning_rate": 4.057971014492754e-05,
      "loss": 0.2488,
      "step": 6600
    },
    {
      "epoch": 48.55072463768116,
      "grad_norm": 4.5006256103515625,
      "learning_rate": 3.8164251207729466e-05,
      "loss": 0.2472,
      "step": 6700
    },
    {
      "epoch": 49.27536231884058,
      "grad_norm": 8.158381462097168,
      "learning_rate": 3.57487922705314e-05,
      "loss": 0.2581,
      "step": 6800
    },
    {
      "epoch": 50.0,
      "grad_norm": 907.0864868164062,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.2517,
      "step": 6900
    },
    {
      "epoch": 50.72463768115942,
      "grad_norm": 166.26844787597656,
      "learning_rate": 3.0917874396135266e-05,
      "loss": 0.2668,
      "step": 7000
    },
    {
      "epoch": 51.44927536231884,
      "grad_norm": 136.55734252929688,
      "learning_rate": 2.8502415458937197e-05,
      "loss": 0.2802,
      "step": 7100
    },
    {
      "epoch": 52.17391304347826,
      "grad_norm": 44.50651550292969,
      "learning_rate": 2.608695652173913e-05,
      "loss": 0.286,
      "step": 7200
    },
    {
      "epoch": 52.89855072463768,
      "grad_norm": 3.917595386505127,
      "learning_rate": 2.3671497584541063e-05,
      "loss": 0.2968,
      "step": 7300
    },
    {
      "epoch": 53.6231884057971,
      "grad_norm": 1.9404659271240234,
      "learning_rate": 2.1256038647342997e-05,
      "loss": 0.2955,
      "step": 7400
    },
    {
      "epoch": 54.34782608695652,
      "grad_norm": 116.66685485839844,
      "learning_rate": 1.8840579710144928e-05,
      "loss": 0.2911,
      "step": 7500
    },
    {
      "epoch": 55.072463768115945,
      "grad_norm": 114.30931854248047,
      "learning_rate": 1.6425120772946863e-05,
      "loss": 0.2874,
      "step": 7600
    },
    {
      "epoch": 55.79710144927536,
      "grad_norm": 96.75677490234375,
      "learning_rate": 1.4009661835748794e-05,
      "loss": 0.2887,
      "step": 7700
    },
    {
      "epoch": 56.52173913043478,
      "grad_norm": 3.5876693725585938,
      "learning_rate": 1.1594202898550725e-05,
      "loss": 0.3006,
      "step": 7800
    },
    {
      "epoch": 57.2463768115942,
      "grad_norm": 289.72052001953125,
      "learning_rate": 9.178743961352658e-06,
      "loss": 0.2787,
      "step": 7900
    },
    {
      "epoch": 57.971014492753625,
      "grad_norm": 102.8034439086914,
      "learning_rate": 6.7632850241545894e-06,
      "loss": 0.2903,
      "step": 8000
    },
    {
      "epoch": 58.69565217391305,
      "grad_norm": 442.1295166015625,
      "learning_rate": 4.347826086956522e-06,
      "loss": 0.2883,
      "step": 8100
    },
    {
      "epoch": 59.42028985507246,
      "grad_norm": 1.9728561639785767,
      "learning_rate": 1.932367149758454e-06,
      "loss": 0.2818,
      "step": 8200
    }
  ],
  "logging_steps": 100,
  "max_steps": 8280,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 60,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.736586980261888e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
